{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS0aUejsau1h"
      },
      "source": [
        "# Lightweight Fine-Tuning Project (Hints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KR-eMsKa7NL"
      },
      "source": [
        "TODO: In this cell, describe your choices for each of the following\n",
        "\n",
        "* PEFT technique: DoRA (Weight-Decomposed Low-Rank Adaptation)\n",
        "* Model: distilbert-base-uncased\n",
        "* Evaluation approach: Accuracy\n",
        "* Fine-tuning dataset: sms_spam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAFDyH9Ea_ox"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FVXgHeK78zJF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
            "autogluon-multimodal 1.2 requires accelerate<1.0,>=0.34.0, but you have accelerate 1.3.0 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
            "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\n",
            "autogluon-timeseries 1.2 requires accelerate<1.0,>=0.34.0, but you have accelerate 1.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Installing required libraries\n",
        "# Note: After running this cell, restart the kernel to ensure the new packages are properly loaded.\n",
        "# Instructions to restart the kernel:\n",
        "# 1. Go to the top menu in Jupyter Notebook and click on \"Kernel\".\n",
        "# 2. From the dropdown, select \"Restart\".\n",
        "# 3. Confirm the restart when prompted.\n",
        "# 4. Wait for the kernel to restart (indicated by the kernel icon becoming active again).\n",
        "# 5. Once the kernel is restarted, continue executing cells from the next one onwards.\n",
        "\n",
        "!pip install -q -U transformers\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U datasets\n",
        "!pip install -q -U peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zJqxvORi9B-X"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-05 21:55:35.198829: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# Suppressing unrelated warnings\n",
        "import torchvision\n",
        "torchvision.disable_beta_transforms_warning()\n",
        "\n",
        "# Necessary imports\n",
        "from datasets import load_dataset\n",
        "from peft import (AutoPeftModelForSequenceClassification,\n",
        "                  LoraConfig,\n",
        "                  get_peft_model,\n",
        "                  TaskType)\n",
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForSequenceClassification,\n",
        "                          DataCollatorWithPadding,\n",
        "                          Trainer,\n",
        "                          TrainingArguments)\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XWTZTkR-9LgD"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sms', 'label'],\n",
              "    num_rows: 4459\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading the sms_spam dataset\n",
        "# Dataset here: https://huggingface.co/datasets/sms_spam\n",
        "# The sms_spam dataset only has a train split, so we use the train_test_split method to split it into train and test\n",
        "dataset = load_dataset(\"sms_spam\", split=\"train\").train_test_split(\n",
        "    test_size=0.2, shuffle=True, seed=23\n",
        ")\n",
        "\n",
        "splits = [\"train\", \"test\"]\n",
        "\n",
        "# View the dataset characteristics\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "j77qHzGX9aRY"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57f8afb56f864eb48495ab4c381b8b54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4459 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12832a6841c5458fb703fb23738e9411",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1115 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO: Load the tokenizer for the \"distilbert-base-uncased\" model.\n",
        "# Hint 1: Use AutoTokenizer.from_pretrained(\"<model-name>\").\n",
        "# Hint 2: The tokenizer will help convert text to tokens that the model can process.\n",
        "#tokenizer = None  # Replace None with the appropriate code.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# TODO: Tokenize the dataset using the loaded tokenizer.\n",
        "# Hint 1: Use the map() function with a lambda to apply tokenization to the \"sms\" column.\n",
        "# Hint 2: Set truncation=True to ensure the sequences fit within the model's maximum input length.\n",
        "tokenized_dataset = {}\n",
        "for split in splits:\n",
        "#    tokenized_dataset[split] = None  # Replace None with the appropriate code.\n",
        "    tokenized_dataset[split] = dataset[split].map(\n",
        "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v9Pepou9bol"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "### Solution for Tokenizer Initialization and Tokenization\n",
        "```python\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "tokenized_dataset = {}\n",
        "for split in splits:\n",
        "    tokenized_dataset[split] = dataset[split].map(\n",
        "        lambda x: tokenizer(x[\"sms\"], truncation=True), batched=True\n",
        "    )\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Wh2b6N6G9f7l"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement a function to compute accuracy from predictions and labels.\n",
        "# Hint 1: Use np.argmax to find the predicted class from the logits.\n",
        "# Hint 2: Compare the predicted classes with the true labels to calculate accuracy.\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = None  # Replace None with np.argmax to get predicted classes.\n",
        "    return {\"accuracy\": None}  # Replace None to compute the mean of correct predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84tNWN1b9s0E"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "### Solution for Metrics Function\n",
        "```python\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {\"accuracy\": (predictions == labels).mean()}\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WEUcg9F09toX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "trainable model parameters: 66955010\n",
            "all model parameters: 66955010\n",
            "percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Loading distilbert-base-uncased for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=2,\n",
        "    id2label={0: \"not spam\", 1: \"spam\"},\n",
        "    label2id={\"not spam\": 0, \"spam\": 1},\n",
        ")\n",
        "\n",
        "# Define a function to print the trainable parameters of the model\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print()\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tx_JPg11-CEB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating the model before fine-tuning...\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create the Trainer class instance to evaluate the pre-trained model on the test set.\n",
        "# Hint 1: Trainer's eval_dataset argument takes an evaluation dataset (i.e. tokenized_dataset[\"test\"]) and computes_metrics argument will take the compute_metrics function callable defined above.\n",
        "# Hint 2: args argument of Trainer will be this --> args=TrainingArguments(output_dir=\"./result-distilbert-base\", per_device_eval_batch_size=4, report_to=\"none\")\n",
        "print(\"Evaluating the model before fine-tuning...\")\n",
        "trainer = None  # Replace with the code given in Hint 2 above\n",
        "pre_finetune_eval = None  # Replace None with the appropriate trainer.evaluate() call.\n",
        "print(pre_finetune_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO3T98c5-G8a"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "### Solution for Model Evaluation Before Fine-Tuning\n",
        "```python\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./result-distilbert-base\",\n",
        "        per_device_eval_batch_size=4,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "pre_finetune_eval = trainer.evaluate()\n",
        "print(pre_finetune_eval)\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nScvRlTUbHqm"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IbVGrvQN-HoH"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Please specify `target_modules` in `peft_config`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      7\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      8\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Replace None with the correct task type.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Get PEFT model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Reduced trainable parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(print_number_of_trainable_model_parameters(peft_model))\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/mapping.py:212\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftMixedModel(model, peft_config, adapter_name\u001b[38;5;241m=\u001b[39madapter_name)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m MODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeftModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    221\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/peft_model.py:176\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    174\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[0;32m--> 176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/tuners/lora/model.py:141\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name, low_cpu_mem_usage: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:184\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:437\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[0m\n\u001b[1;32m    433\u001b[0m _has_modules_to_save \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    435\u001b[0m model_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model_config(model)\n\u001b[0;32m--> 437\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_adapter_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model(peft_config, model)\n\u001b[1;32m    440\u001b[0m key_list \u001b[38;5;241m=\u001b[39m [key \u001b[38;5;28;01mfor\u001b[39;00m key, _ \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules()]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/peft/tuners/lora/model.py:488\u001b[0m, in \u001b[0;36mLoraModel._prepare_adapter_config\u001b[0;34m(peft_config, model_config)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mtarget_modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING:\n\u001b[0;32m--> 488\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify `target_modules` in `peft_config`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     peft_config\u001b[38;5;241m.\u001b[39mtarget_modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    490\u001b[0m         TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    491\u001b[0m     )\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m peft_config\n",
            "\u001b[0;31mValueError\u001b[0m: Please specify `target_modules` in `peft_config`"
          ]
        }
      ],
      "source": [
        "# TODO: Complete the LoRA configuration.\n",
        "# Hint 1: Use r=16, lora_alpha=16, and lora_dropout=0.05 as default values.\n",
        "# Hint 2: Specify target_modules where LoRA is applied (e.g., MultiHeadAttention layers) i.e. target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"]\n",
        "# Hint 3: Use use_dora=True\n",
        "# Hint 4: Use task_type=TaskType.SEQ_CLS for sequence classification tasks.\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=None,  # Replace None with the list of target modules.\n",
        "    bias='none',\n",
        "    task_type=None  # Replace None with the correct task type.\n",
        ")\n",
        "\n",
        "# Get PEFT model\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Reduced trainable parameters\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY8HRnB6_ASP"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "### Solution for LoRA Configuration\n",
        "```python\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\", \"out_lin\"],\n",
        "    bias='none',\n",
        "    use_dora=True,\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq2sm5DC_GYI"
      },
      "outputs": [],
      "source": [
        "# Training and evaluating the model prepared for PEFT\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./result-distilbert-lora\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        save_strategy=\"epoch\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_steps=1,\n",
        "        num_train_epochs=2,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lTVwryM_XzI"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model after fine-tuning\n",
        "print(\"Evaluating the model after fine-tuning...\")\n",
        "post_finetune_eval = trainer.evaluate()\n",
        "print(post_finetune_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzywX_xD_pVn"
      },
      "outputs": [],
      "source": [
        "# TODO: Save the fine-tuned model to a directory.\n",
        "# Hint: Use model.save_pretrained(\"<directory-name>\").\n",
        "peft_model.save_pretrained(None)  # Replace None with the directory path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZCfbX0_s3H"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "### Solution for Saving the Model\n",
        "```python\n",
        "peft_model.save_pretrained(\"distilbert-lora\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aL2hum9bY3G"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yl7A8f6C_wUv"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the saved model from the directory.\n",
        "# Hint: Use AutoPeftModelForSequenceClassification.from_pretrained(\"<directory-name>\").\n",
        "lora_model = None  # Replace None with the appropriate code to load the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdylTntACmB"
      },
      "source": [
        "<details>\n",
        "<summary>Click to see the answer</summary>\n",
        "\n",
        "### Solution for Loading the Model\n",
        "```python\n",
        "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distilbert-lora\")\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6fe3q2rADSN"
      },
      "outputs": [],
      "source": [
        "# Evaluate the loaded fine-tuned model\n",
        "loaded_trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./result-distilbert-lora\",\n",
        "        per_device_eval_batch_size=4,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Evaluating the loaded fine-tuned model...\")\n",
        "loaded_model_eval = loaded_trainer.evaluate()\n",
        "print(loaded_model_eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m0AzDyuFXc1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
