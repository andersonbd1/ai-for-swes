{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "In this cell, my choices are described:\n",
    "\n",
    "* PEFT technique: DoRA: \"By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead.\"\n",
    "* Model: ModernBERT: \"In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.\"\n",
    "* Evaluation approach: Binary Classification somewhat evenly split, so we'll use F1 to compare\n",
    "* Fine-tuning dataset: https://huggingface.co/datasets/Jacobvs/PoliticalTweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (after running this cell restart the Session/Kernel)\n",
    "!pip install transformers datasets peft accelerate optuna -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 17:07:09.975664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    precision_recall_fscore_support\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "MODEL_ID = \"answerdotai/ModernBERT-base\"\n",
    "DATASET_NAME = \"Jacobvs/PoliticalTweets\"\n",
    "ADAPTER_PATH = \"./modernbert-political-dora-best\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch._dynamo.config.disable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load and prepare dataset\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad1636de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Initialize model components\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"Republican\", 1: \"Democrat\"}\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3.1 Tokenize the dataset before evaluation\n",
    "def tokenize_function(examples):\n",
    "    # Remove return_tensors='pt' so HF Datasets keeps outputs as lists\n",
    "    # (which the DataCollatorWithPadding will batch into tensors)\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = split_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"index, date, id, username, text, party\"]\n",
    ")\n",
    "\n",
    "# 3.2 Evaluate base model before fine-tuning\n",
    "def evaluate_model(model, dataset_split, split_name=\"Test\"):\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Evaluating {split_name} Set\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Define evaluation arguments\n",
    "    eval_args = TrainingArguments(\n",
    "        output_dir=\"./pre_fine_tuning_results\",\n",
    "        report_to=\"none\"  # Disable all logging integrations, including wandb\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        args=eval_args,\n",
    "        model=model,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "        processing_class=tokenizer\n",
    "    )\n",
    "\n",
    "    outputs = trainer.predict(dataset_split)\n",
    "    predictions = np.argmax(outputs.predictions, axis=1)\n",
    "    labels = outputs.label_ids\n",
    "\n",
    "    print(classification_report(labels, predictions, target_names=[\"Republican\", \"Democrat\"]))\n",
    "    plot_confusion_matrix(labels, predictions)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=[\"Republican\", \"Democrat\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "# Pre-training evaluation on the tokenized dataset\n",
    "evaluate_model(base_model, tokenized_dataset[\"test\"], \"Base Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Optimized DoRA configuration\n",
    "dora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"Wqkv\", \"Wi\", \"Wo\"],\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    modules_to_save=[\"classifier\", \"model.final_norm\"],\n",
    "    use_dora=True,\n",
    "    init_lora_weights=\"gaussian\",\n",
    "    inference_mode=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Apply DoRA adapters\n",
    "model = get_peft_model(base_model, dora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=ADAPTER_PATH,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=8,\n",
    "    weight_decay=0.1,\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_accumulation_steps=1,\n",
    "    group_by_length=True,\n",
    "    dataloader_num_workers=2,\n",
    "    torch_compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Metrics calculation\n",
    "def compute_metrics(p):\n",
    "    predictions = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    # Compute MACRO metrics only (or MICRO if you prefer),\n",
    "    # and keep overall accuracy. This greatly simplifies your logs.\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    accuracy = (predictions == labels).mean()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": precision,\n",
    "        \"macro_recall\": recall,\n",
    "        \"macro_f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Early stopping\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86b2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Run training\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302716fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Save and load best model\n",
    "final_model_path = \"./modernbert-political-dora-final\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PeftModel.from_pretrained(base_model, final_model_path)\n",
    "model = model.merge_and_unload().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Post-training evaluation\n",
    "print(\"\\nEvaluating Fine-tuned Model on Test Set\")\n",
    "evaluate_model(model, tokenized_dataset[\"test\"], \"Fine-tuned Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 14. Error analysis\n",
    "test_output = trainer.predict(tokenized_dataset[\"test\"])\n",
    "test_preds = np.argmax(test_output.predictions, axis=1)\n",
    "test_labels = test_output.label_ids\n",
    "\n",
    "misclassified = np.where(test_preds != test_labels)[0]\n",
    "if len(misclassified) > 0:\n",
    "    print(\"\\nError Analysis:\")\n",
    "    for idx in misclassified[:3]:\n",
    "        # Cast `idx` to a Python int\n",
    "        original_text = split_dataset[\"test\"][int(idx)][\"sms\"]\n",
    "        print(f\"\\nError Case {idx+1}:\")\n",
    "        print(f\"Text: {original_text[:200]}{'...' if len(original_text)>200 else ''}\")\n",
    "        print(f\"True: {model.config.id2label[test_labels[idx]]}\")\n",
    "        print(f\"Predicted: {model.config.id2label[test_preds[idx]]}\")\n",
    "else:\n",
    "    print(\"\\nPerfect classification on test set!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
